{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Autoencoders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/CAMM-UTK/acns-AI-tutorial/blob/main/Intro_Unsupervised_Learning/02_Autoencoders.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Some setup necessary to run in a co-lab environment and get the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install zenodo_get\n",
    "!zenodo_get --doi=10.5281/zenodo.12174462\n",
    "!tar -xzf ./unsupervised_acns_ai_tutorial.tar.gz\n",
    "\n",
    "%pip install git+https://github.com/agdelma/ml4s.git#egg=ml4s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "sys.path.append('./include')\n",
    "\n",
    "import ml4s\n",
    "ml4s.set_css_style('./include/bootstrap.css')\n",
    "\n",
    "%config InlineBackend.figure_format = 'svg'\n",
    "plt.style.use('./include/notebook.mplstyle')\n",
    "np.set_printoptions(linewidth=120)\n",
    "colors = plt.rcParams['axes.prop_cycle'].by_key()['color']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Previously\n",
    "\n",
    "- Principal Component Analysis\n",
    "- Identifying the low-dimensional latent space which maximally explains the *variance* of the data\n",
    "- Implementing PCA by hand and with `sklearn`\n",
    "\n",
    "## Now\n",
    "\n",
    "- Connection between PCA and autoencoders, a compressive deep neural network architecture\n",
    "- Application of PCA for clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ml4s.draw_network([12,8,4,2,4,8,12],annotate=False )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Principal Component Analysis (PCA)\n",
    "\n",
    "Recall that for a given set of unlabelled data $\\{ \\boldsymbol{x}^{(n)} \\}_{n=1}^{N}$ our goal is to project the data onto a latent space having dimensionality $M < D$.  We did this by performing a spectral decomposition of the covariance matrix\n",
    "\n",
    "\\begin{equation}\n",
    "\\Sigma(\\mathbf{X}) = \\frac{1}{N-1} \\mathbf{X}^{\\top}\\mathbf{X}\n",
    "\\end{equation}\n",
    "\n",
    "where $\\mathbf{X}$ is the  data design matrix: \n",
    "\n",
    "\\begin{equation}\n",
    "\\mathbf{X} = \\left( \\begin{array}{cccc}\n",
    "        x_{1}^{(1)} & x_{2}^{(1)} & \\cdots & x_{D}^{(1)} \\\\\n",
    "\\vdots        &      \\vdots    & \\ddots & \\vdots \\\\\n",
    "        x_{1}^{(N)} & x_{2}^{(N)} & \\cdots & x_{D}^{(N)} \\\\\n",
    "\\end{array}\n",
    "\\right)\\, .\n",
    "\\end{equation}\n",
    "\n",
    "We determined:\n",
    "\n",
    "\\begin{equation}\n",
    "\\boldsymbol{V}^\\top \\Sigma(\\mathbf{X}) \\boldsymbol{V} = \\Lambda\n",
    "\\end{equation}\n",
    "\n",
    "where $\\Lambda_{ij} = \\lambda_i \\delta_{ij}$ is the diagonal matrix of principle components and the PCA vectors are encoded as the columns of the orthogonal matrix $\\boldsymbol{V}$.\n",
    "\n",
    "Also recall the *percentage of the explained variance* defined:\n",
    "\n",
    "\\begin{equation}\n",
    "\\text{PCA-j} = \\frac{\\lambda_j}{\\sum_{j=1}^{D} \\lambda_j}\n",
    "\\end{equation}\n",
    "\n",
    "and the projector:\n",
    "\n",
    "\\begin{equation}\n",
    "\\boldsymbol{P} = \\sum_{j=1}^M\\boldsymbol{v}_j\\boldsymbol{v}_j^\\mathsf{T}\\, .\n",
    "\\end{equation}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Networks and Linear Autoencoders\n",
    "\n",
    "There is a very nice way to interpret PCA as a type of *linear autoencoder* whereby one trains a neural network with a hidden layer (with linear activation) that acts as an **information bottleneck.**  We want to minimize the least squred error between input and output. The network calculates:\n",
    "\n",
    "\\begin{equation}\n",
    "\\boldsymbol{P} \\mathbf{x}^{(n)}\n",
    "\\end{equation}\n",
    "\n",
    "for each $\\mathbf{x}_n$ and we minimize the cost:\n",
    "\n",
    "\\begin{equation}\n",
    "\\mathcal{C} = \\left \\langle \\mathbf{x}^{(n) \\top}  \\mathbf{x}^{(n)} - \\mathbf{x}^{(n)\\top} \\boldsymbol{P}\\mathbf{x}^{(n)} \\right \\rangle  = \\frac{1}{N} \\sum_{n=1}^{N}\\left( \\mathbf{x}^{(n) \\top} \\mathbf{x}^{(n)} - \\mathbf{x}^{(n) \\top} \\boldsymbol{P}\\mathbf{x}^{(n)} \\right) \\, .\n",
    "\\end{equation}\n",
    "\n",
    "To obtain the 1st princpal component for our example above we consider the linear autoencoder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ml4s.draw_network([2,1,2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup a GPU device (if available)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f'You are running on: {device}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a simple linear autoencoder in `pytorch`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearAutoencoder(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim):\n",
    "        super(LinearAutoencoder, self).__init__()\n",
    "        self.encoder = nn.Linear(input_dim, hidden_dim)\n",
    "        self.decoder = nn.Linear(hidden_dim, input_dim)\n",
    "        self.hidden = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        self.hidden = self.encoder(x)\n",
    "        x = self.decoder(self.hidden)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set some hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim = 2\n",
    "latent_dim = 1\n",
    "learning_rate = 0.001\n",
    "num_epochs = 100\n",
    "batch_size = 32"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.loadtxt('./data/scatter_2d_pca.dat').astype(np.float32)\n",
    "dataset = torch.tensor(x).to(device)\n",
    "\n",
    "# DataLoader\n",
    "train_loader = torch.utils.data.DataLoader(dataset=dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Initialization\n",
    "model = LinearAutoencoder(input_dim, latent_dim).to(device)\n",
    "\n",
    "# Validation using MSE Loss function\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "# Using an Adam Optimizer\n",
    "optimizer = torch.optim.Adam(model.parameters(),lr=learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "losses = []\n",
    "for epoch in range(num_epochs):\n",
    "    for batch in train_loader:\n",
    "        # Forward pass\n",
    "        outputs = model(batch)\n",
    "        loss = criterion(outputs, batch)\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    losses.append(loss.item())\n",
    "    # Print loss every 10 epochs\n",
    "    if (epoch+1) % 10 == 0:\n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')\n",
    "\n",
    "print('Training complete')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model\n",
    "with torch.no_grad():\n",
    "    outputs = model(dataset)\n",
    "    final_loss = criterion(outputs, dataset)\n",
    "    \n",
    "    plt.plot(losses,color=colors[0], linestyle='-',label=f'cost = {final_loss.item():.2f}')\n",
    "    plt.ylabel(\"Cost\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract the trained weights and biases from the model\n",
    "\n",
    "The weights are in the format [`decoder_weight, decoder_bias, encoder_weight, encoder_bias]`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = [param.detach().numpy() for param in model.parameters()]\n",
    "ml4s.draw_network([input_dim,latent_dim,input_dim], weights=[weights[0],weights[2]], \n",
    "                           biases=[weights[1],weights[3]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot the raw data with the learned decoder weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(x[:,0], x[:,1], s=1, alpha=0.5, label='data')\n",
    "_x = np.linspace(-4, 4, 100)\n",
    "\n",
    "decoder_weight = weights[2]\n",
    "plt.plot(_x, decoder_weight[1, 0] / decoder_weight[0, 0] * _x, '-', color=colors[0], label=r'$\\mathbf{w}_1$')\n",
    "\n",
    "plt.axis('equal')\n",
    "plt.xticks([])\n",
    "plt.yticks([]);\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Can you get more principal components with this strategy? \n",
    "\n",
    "Unlike the eigenvector problem we solved for PCA, the issue is that there is no guarentee the components will be orthognoal.  See:\n",
    "\n",
    "[E. Plaut, From Principal Subspaces to Principal Components with Linear Autoencoders, arXiv:1804.10253 (2018)](https://arxiv.org/abs/1804.10253)\n",
    "\n",
    "for a discussion of how you can re-orthogonalize via a singular value decomposition of the weight matrix.\n",
    "\n",
    "### Let's see what happens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Initialization\n",
    "latent_dim = 2\n",
    "model = LinearAutoencoder(input_dim, latent_dim).to(device)\n",
    "\n",
    "# Validation using MSE Loss function\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "# Using an Adam Optimizer with lr = 0.1\n",
    "optimizer = torch.optim.Adam(model.parameters(),lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "losses = []\n",
    "for epoch in range(num_epochs):\n",
    "    for batch in train_loader:\n",
    "        # Forward pass\n",
    "        outputs = model(batch)\n",
    "        loss = criterion(outputs, batch)\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    losses.append(loss.item())\n",
    "    # Print loss every 10 epochs\n",
    "    if (epoch+1) % 10 == 0:\n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')\n",
    "\n",
    "print('Training complete')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model\n",
    "with torch.no_grad():\n",
    "    outputs = model(dataset)\n",
    "    final_loss = criterion(outputs, dataset)\n",
    "\n",
    "plt.plot(losses,color=colors[0], linestyle='-',label=f'cost = {final_loss.item():.2f}')\n",
    "plt.ylabel(\"Cost\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot the final weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = [param.detach().numpy() for param in model.parameters()]\n",
    "decoder_weight = weights[2]\n",
    "\n",
    "ml4s.draw_network([input_dim,latent_dim,input_dim], weights=[weights[0],weights[2]], \n",
    "                           biases=[weights[1],weights[3]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot the data with the learned (non-orghgonal) latent space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots()\n",
    "ax.scatter(x[:,0],x[:,1], s=1, alpha=0.5,label='data')\n",
    "_x = np.linspace(-1,1,100)\n",
    "\n",
    "ax.plot(_x,decoder_weight[1, 0] / decoder_weight[0, 0] * _x, '-', color=colors[0], label=r'$\\mathbf{w}_1$')\n",
    "ax.plot(_x,decoder_weight[1, 1] / decoder_weight[0, 1] * _x, '-', color=colors[-2], label=r'$\\mathbf{w}_2$')\n",
    "\n",
    "ax.axis('equal')\n",
    "ax.set_xticks([])\n",
    "ax.set_yticks([])\n",
    "ax.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "<div class=\"span alert alert-success\">\n",
    "<h2>Use Case: Detecting a Phase Transition in the 2D Ising Model </h2>\n",
    "\n",
    "In a recent <a href=\"https://journals.aps.org/pre/abstract/10.1103/PhysRevE.107.054104\" title=\"Group-equivariant autoencoder for identifying spontaneously broken symmetries\">paper</a> we used a group-equivariant autoencoder for identifying a phase transition. In this exercise you will confirm our analysis.\n",
    "    \n",
    "1. Load Ising model configurations from disk and investigate the configurations.\n",
    "2. Define a non-linear autoencoder model and train.\n",
    "3. Investigate the properties of the learned latent space representation.\n",
    "4. Use known labels to confirm that we identify a ferromagnetic phase transition.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the data from disk\n",
    "\n",
    "We have a $2000$ Ising model configurations for a $80 \\times 80$ square lattice at various temperatures obtained via Monte Carlo sampling.  Without the temperature information we can think of this as unlabeled data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "L = 80\n",
    "configs = np.loadtxt(f'./data/Ising2D_config_L{L}.dat.gz')\n",
    "temps = np.loadtxt(f'./data/Ising2D_temps_L{L}.dat')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot a random configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots(ncols=1,nrows=1,figsize=(4,4))\n",
    "\n",
    "idx = np.random.randint(low=0,high=configs.shape[0]+1)\n",
    "img = ax.matshow(configs[idx].reshape(L,L), cmap='binary')\n",
    "ax.set_xticks([])\n",
    "ax.set_yticks([])\n",
    "\n",
    "ax.set_title(f'$T = {temps[idx]:.1f}J$');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define a non-linear autoencoder "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Autoencoder(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, latent_dim):\n",
    "        super(Autoencoder, self).__init__()\n",
    "        # Encoder\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Linear(hidden_dim,latent_dim),\n",
    "            nn.LeakyReLU()\n",
    "        )\n",
    "        # Decoder\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(latent_dim,hidden_dim),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Linear(hidden_dim,input_dim)\n",
    "        )\n",
    "        self.hidden = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        self.hidden = self.encoder(x)\n",
    "        x = self.decoder(self.hidden)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim = configs.shape[1]\n",
    "hidden_dim = 32\n",
    "latent_dim = 2\n",
    "learning_rate = 1e-5\n",
    "num_epochs = 200\n",
    "batch_size = 32"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create the dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = torch.tensor(configs.astype(np.float32)).to(device)\n",
    "train_loader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Initialization\n",
    "model = Autoencoder(input_dim,hidden_dim,latent_dim).to(device)\n",
    "\n",
    "# Validation using MSE Loss function\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "# Using an Adam Optimizer\n",
    "optimizer = torch.optim.Adam(model.parameters(),lr=learning_rate,weight_decay = 1e-8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "losses = []\n",
    "for epoch in range(num_epochs):\n",
    "    for batch in train_loader:\n",
    "        # Forward pass\n",
    "        outputs = model(batch)\n",
    "        loss = criterion(outputs, batch)\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    losses.append(loss.item())\n",
    "    # Print loss every 10 epochs\n",
    "    if (epoch+1) % 10 == 0:\n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')\n",
    "\n",
    "print('Training complete')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Investigate the training history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model\n",
    "with torch.no_grad():\n",
    "    outputs = model(dataset)\n",
    "    final_loss = criterion(outputs, dataset)\n",
    "    \n",
    "plt.plot(losses,color=colors[0], linestyle='-',label=f'cost = {final_loss.item():.2f}')\n",
    "plt.ylabel(\"Cost\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot the latent-space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    latent_x = model.hidden.cpu().numpy()\n",
    "    \n",
    "fig,ax = plt.subplots(1)\n",
    "ax.scatter(latent_x[:,0],latent_x[:,1], s=5)\n",
    "ax.set_ylabel(r'$latent-2$')\n",
    "ax.set_xlabel(r'$latent-1$')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We actually have labels!\n",
    "\n",
    "We know the temperatures where each configuration was measured, so we can add temperature labels to the dat to see what the latent space is doing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots(1)\n",
    "sc = ax.scatter(latent_x[:,0],latent_x[:,1], s=5, c=temps, cmap='Spectral_r')\n",
    "ax.set_ylabel(r'$latent-2$')\n",
    "ax.set_xlabel(r'$latent-1$')\n",
    "\n",
    "fig.colorbar(sc, ax=ax, label='Temperature / J')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Investigate what we have learned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots(1)\n",
    "ax.plot(temps,latent_x[:,0], 'o', ms=1, alpha=0.1, label='Raw Autoencoder')\n",
    "ax.set_xlabel('Temperature $T/J$')\n",
    "ax.set_ylabel('Magnetization')\n",
    "ax.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"span alert alert-info\" role=\"alert\">\n",
    "    <h3>We have discovered something that looks like the magnetization!</h3>\n",
    "</div>\n",
    "\n",
    "However, note, there is no *physics* in our autoencoder.  We have not input the fact that the magnetization needs to be normalized to the 1. Let's do this and compare with the exact magnetization:\n",
    "\n",
    "\\begin{equation}\n",
    "m=\\left[1-\\left(\\sinh \\frac{2J}{k_{\\rm B}T}\\right)^{-4}\\right]^{\\frac {1}{8}}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def magnetization_exact_(T):\n",
    "    '''We use units where J/k_B = 1.'''\n",
    "    Tc = 2.0/np.log(1.0+np.sqrt(2.0))\n",
    "    if T < Tc:\n",
    "        return (1.0 - np.sinh(2.0/T)**(-4))**(1.0/8)\n",
    "    else:\n",
    "        return 0.0\n",
    "magnetization_exact = np.vectorize(magnetization_exact_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalize the raw latent space values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "latent_m = latent_x[:,0]\n",
    "m = np.zeros_like(latent_m)\n",
    "\n",
    "# find the maximum value and normalize\n",
    "idx_pos = np.where(latent_m>0)\n",
    "m[idx_pos] = latent_m[idx_pos]/np.max(latent_m)\n",
    "\n",
    "# find the minimum value and normalize\n",
    "idx_neg = np.where(latent_m<0)\n",
    "m[idx_neg] = latent_m[idx_neg]/np.min(latent_m)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot and Compare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot and compare\n",
    "fig,ax = plt.subplots(1)\n",
    "\n",
    "ax.plot(temps,m, 'o', ms=2, alpha=0.1, label='Norm. Autoencoder')\n",
    "_T = np.linspace(1,3,1000)\n",
    "ax.plot(_T,magnetization_exact(_T), color=colors[0], zorder=-1, lw=1, label='Exact')\n",
    "\n",
    "ax.set_xlabel('Temperature $T/J$')\n",
    "ax.set_ylabel('Magnetization')\n",
    "\n",
    "ax.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"span alert alert-info\" role=\"alert\">\n",
    "    <h3>It Works!</h3>\n",
    "</div>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
