{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Unsupervised Learning "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/CAMM-UTK/acns-AI-tutorial/blob/main/Intro_Unsupervised_Learning/01_Unsupervised_Learning.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Some setup necessary to run in a co-lab environment and get the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install zenodo_get\n",
    "!zenodo_get --doi=10.5281/zenodo.12174462\n",
    "!tar -xzf ./unsupervised_acns_ai_tutorial.tar.gz\n",
    "\n",
    "%pip install git+https://github.com/agdelma/ml4s.git#egg=ml4s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "sys.path.append('./include')\n",
    "\n",
    "import ml4s\n",
    "ml4s.set_css_style('./include/bootstrap.css')\n",
    "\n",
    "%config InlineBackend.figure_format = 'svg'\n",
    "plt.style.use('./include/notebook.mplstyle')\n",
    "np.set_printoptions(linewidth=120)\n",
    "colors = plt.rcParams['axes.prop_cycle'].by_key()['color']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Until Now\n",
    "\n",
    "### Supervised Learning\n",
    "\n",
    "- We have considered learning tasks (regression, classification) where there was labelled data and we could train a neural network to \"learn\" these features of the data. \n",
    "- This is incredibly useful, but many problems in the physical sciences aren't necessarily about prediciton. We usually want to **learn** something about the underlying distribution that generated the observation. \n",
    "- It is not useful in physics to make good predictions with the wrong model. \n",
    "\n",
    "### Now - Unsupervised Learning\n",
    "\n",
    "- Our first foray into unsupervised learning, a large and exciting field.\n",
    "- Concerned with discovering structure in unalabelled data.\n",
    "- Dimensional reduction, clustering, generative models.\n",
    "- We will begin with dimensional reduction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_data(ax, title, m1, m2, c1=colors[0], c2=colors[-2]):\n",
    "    y1 = np.random.multivariate_normal((1, 1), np.diag([0.1, 0.1]), 10)\n",
    "    y2 = np.random.multivariate_normal((3, 3), np.diag([0.1, 0.1]), 10)\n",
    "    ax.plot(y1[:, 0], y1[:, 1], m1, mfc='None', mec=c1, ms=8)\n",
    "    ax.plot(y2[:, 0], y2[:, 1], m2, mfc='None', mec=c2, ms=8)\n",
    "    ax.set_title(title)\n",
    "    ax.set_xlim(-1, 5)\n",
    "    ax.set_ylim(-1, 5)\n",
    "    ax.set_xlabel('$x_1$')\n",
    "    ax.set_ylabel('$x_2$')\n",
    "    ax.spines[['right', 'top']].set_visible(False)\n",
    "    ax.set_xticks([])\n",
    "    ax.set_yticks([])\n",
    "    \n",
    "fig, ax = plt.subplots(1, 2, figsize=(8, 4))\n",
    "\n",
    "plot_data(ax[0], 'Supervised Learning', 'o', 'x')\n",
    "ax[0].legend(['label 1', 'label 2'], loc='upper left')\n",
    "\n",
    "plot_data(ax[1], 'Unsupervised Learning', 'o', 'o', c2=colors[0])\n",
    "\n",
    "from matplotlib.patches import Arc\n",
    "with plt.xkcd(scale=5):\n",
    "    arc_params = dict(angle=360, theta1=0, theta2=350, edgecolor='#ff575a', lw=2, facecolor='none', alpha=0.5, zorder=1)\n",
    "    ax[1].add_patch(Arc((1, 1), 2.2, 2.2, **arc_params))\n",
    "    ax[1].add_patch(Arc((3, 3), 2.2, 2.2, **arc_params))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dimensional Reduction\n",
    "\n",
    "Most of our data sets live in a very high dimension (*e.g.* consider a $30\\times 30$ Ising model, configurations live in 900 dimensional space of binary numbers!)  Our main goal will be to project (or embed) high dimensional data (observations) into a lower dimensional space where it can be better analyzed / understood.   \n",
    "\n",
    "This subspace is called the **latent space**.\n",
    "\n",
    "Let's consider a very simple example of points in two spatial dimensions.\n",
    "\n",
    "<!--\n",
    "N = 2000\n",
    "x = np.random.normal(loc=[0,0],scale=[1,0.4], size=[N,2])\n",
    "\n",
    "θ = np.radians(35)\n",
    "R = np.array([[np.cos(θ), -np.sin(θ)], [np.sin(θ), np.sin(θ)]])\n",
    "for i in range(N):\n",
    "    x[i] = R @ x[i]\n",
    "np.savetxt('../data/scatter_2d_pca.dat',x)\n",
    "-->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.loadtxt('./data/scatter_2d_pca.dat')\n",
    "plt.scatter(x[:,0],x[:,1], s=1)\n",
    "plt.axis('equal')\n",
    "plt.xticks([])\n",
    "plt.yticks([]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Principal Component Analysis (PCA)\n",
    "\n",
    "The goal of PCA is to identify the directions of largest variance with the intuition that these correspond to **signal**, while any orthogonal spread in the data is due to **noise**.  It has an equivalent definition in terms of the linear projection that minimizes the average projection cost (mean squared deviation between original vector and its deviation).\n",
    "\n",
    "\n",
    "Consider the set of observations $\\{ \\boldsymbol{x}^{(n)} \\}_{n=1}^{N}$ where, unlike the supervised case,  we do not have any associated targets or *labels* $y^{(n)}$.  Each $\\boldsymbol{x}^{(n)} \\in \\mathbb{R}^D$ lives in a D-dimensional feature space.  \n",
    "\n",
    "Without loss of generality, we assume that the mean of the data set is zero: \n",
    "\n",
    "\\begin{equation}\n",
    "\\langle \\boldsymbol{x} \\rangle = \\frac{1}{N} \\sum_{n=1}^{N} \\boldsymbol{x}^{(n)} = (0,\\dots,0).\n",
    "\\end{equation}\n",
    "\n",
    "If this is not the case, we can simply subtract the mean from each data point.  \n",
    "\n",
    "**Goal:** project the data onto a latent space having dimensionality $M < D$.\n",
    "\n",
    "To begin, let's project onto a 1-dimenisional space, i.e. $M=1$.  The direction of this dimension (i.e. its unit vector) can be described by a single vector, $\\boldsymbol{v}_1$, which we assume has unit norm: $\\boldsymbol{v}_1 \\cdot \\boldsymbol{v}_1 = 1$.  We then project each data point, $\\boldsymbol{x}^{(n)}$ onto $\\boldsymbol{v}_1$ via $\\boldsymbol{v}_1^\\top \\boldsymbol{x}^{(n)}$. \n",
    "\n",
    "The variance of the resulting data set is given by:\n",
    "\n",
    "\\begin{align}\n",
    "\\sigma^2 \\equiv \\bigg \\langle \\big\\lvert\\boldsymbol{v}_1^{\\top} \\boldsymbol{x}^{(n)} - \\boldsymbol{v}_1^{\\top}\\underbrace{\\langle  \\boldsymbol{x} \\rangle}_{0} \\big\\rvert^2 \\bigg \\rangle  & = \\frac{1}{N-1} \\sum_{n=1}^{N} \\boldsymbol{v}_1^{\\top}  \\boldsymbol{x}^{(n)}  \\boldsymbol{x}^{(n)\\top}  \\boldsymbol{v}_1 \\\\\n",
    "& = \\boldsymbol{v}_1^{\\top} \\Sigma(\\mathbf{X}) \\boldsymbol{v}_1\n",
    "\\end{align}\n",
    "\n",
    "where we have defined\n",
    "\n",
    "\\begin{equation}\n",
    "\\Sigma(\\mathbf{X}) = \\frac{1}{N-1} \\mathbf{X}^{\\top}\\mathbf{X}\n",
    "\\end{equation}\n",
    "\n",
    "to be the $D \\times D$ covariance matrix of the data design matrix: \n",
    "\n",
    "\\begin{equation}\n",
    "\\mathbf{X} = \\left( \\begin{array}{cccc}\n",
    "        x_{1}^{(1)} & x_{2}^{(1)} & \\cdots & x_{D}^{(1)} \\\\\n",
    "\\vdots        &      \\vdots    & \\ddots & \\vdots \\\\\n",
    "        x_{1}^{(N)} & x_{2}^{(N)} & \\cdots & x_{D}^{(N)} \\\\\n",
    "\\end{array}\n",
    "\\right)\\, .\n",
    "\\end{equation}\n",
    "\n",
    "\n",
    "We now want to maximize the variance subject to the constraint $\\boldsymbol{v}_1 \\cdot \\boldsymbol{v}_1 = 1$ which we can do by adding a Lagrange multiplier $\\lambda_1$:\n",
    "\n",
    "\\begin{align}\n",
    "%\\frac{\\partial}{\\partial \\boldsymbol{v}_1^{\\top}} \n",
    "\\boldsymbol{\\nabla}_{\\boldsymbol{v}_1^{\\top}} \\left[\\boldsymbol{v}_1^{\\top} \\Sigma(\\mathbf{X}) \\boldsymbol{v}_1 + \\lambda_1 (1-\\boldsymbol{v}_1^{\\top} \\cdot \\boldsymbol{v}_1) \\right] & = \\Sigma(\\mathbf{X}) \\boldsymbol{v}_1 - \\lambda_1 \\boldsymbol{v}_1 = 0 \\\\\n",
    "&\\Rightarrow  \\Sigma(\\mathbf{X})\\boldsymbol{v}_1 = \\lambda_1 \\boldsymbol{v}_1\n",
    "\\end{align}\n",
    "\n",
    "which tells us that $\\boldsymbol{v}_1$ is a right eigenvector of $\\Sigma(\\mathbf{X})$.  If we multiple on the left by $\\boldsymbol{v}_1^{\\top}$ and utilize the unit norm we find:\n",
    "\n",
    "\\begin{equation}\n",
    "\\boxed{\\lambda_1 = \\boldsymbol{v}_1^{\\top} \\Sigma(\\mathbf{X}) \\boldsymbol{v}_1.}\n",
    "\\end{equation}\n",
    "\n",
    "Thus the variance will be maximized when $\\boldsymbol{v}_1$ is chosen to be the the eigenvector with largest eignvalue $\\lambda_1$.  It is known as the **first principle component**.  \n",
    "\n",
    "Not surprisingly, we can determine additional principal components in an iterative fashion which maximizes the projected variance amongst all possible directions orthogonal to the ones already considered.  This identifies the $M$ principal components as the $M$ eigenvectors corresponding to the $M$ largest eigenvalues $\\boldsymbol{v}_j$ of $\\Sigma(\\mathbf{X})$.\n",
    "\n",
    "In practice, we simply diagonlize the symmetrix square matrix: $\\Sigma(\\mathbf{X}) = (N-1)^{-1}\\mathbf{X}^{\\top}\\mathbf{X}$, the principle components are the eigenvalues $\\lambda_j$ and we organize the eigenvectors into a column matrix $\\boldsymbol{V}$.\n",
    "\n",
    "We can do this with the `scipy.linalg` package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.linalg\n",
    "N = x.shape[0]\n",
    "x -= np.average(x,axis=0)\n",
    "Σ = x.T @ x / (N-1)\n",
    "λ,V = scipy.linalg.eigh(Σ)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"span alert alert-warning\">\n",
    "    Note: <code>eigh()</code> returns eigenvalues in ascending order! We need to re-order to get the maximum eignvalues.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "λ = λ[::-1]\n",
    "V = np.flip(V,axis=1)\n",
    "\n",
    "print(f'λ = {λ}')\n",
    "print(f'V = {V}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The columns of $\\mathbf{V}$ correspond to the *unit-vectors* that span our latent space.\n",
    "\n",
    "\\begin{equation}\n",
    "\\mathbf{V} = \\left( \\begin{array}{cc}\n",
    "        v_{1,1} & v_{1,2} \\\\\n",
    "        v_{2,1} & v_{2,2} \\\\\n",
    "\\end{array}\n",
    "\\right)\\, .\n",
    "\\end{equation}\n",
    "\n",
    "### Percentage of the Explained Variance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An important quantity is the *percentage of the explained variance* defined by:\n",
    "\n",
    "\\begin{equation}\n",
    "\\text{PCA-j} = \\frac{\\lambda_j}{\\sum_{j=1}^{D} \\lambda_j}\n",
    "\\end{equation}\n",
    "\n",
    "We can plot the resulting axes defined by the principal components, or alternatively, can define a projection operator (matrix):\n",
    "\n",
    "\\begin{equation}\n",
    "\\boldsymbol{P} = \\sum_{j=1}^M\\boldsymbol{v}_j\\boldsymbol{v}_j^\\mathsf{T}\n",
    "\\end{equation}\n",
    "\n",
    "to plot with respect to the new axes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots(1,2,figsize=(8,4))\n",
    "\n",
    "ax[0].scatter(x[:,0],x[:,1], s=1, alpha=0.5)\n",
    "\n",
    "_x = np.linspace(-4,4,100)\n",
    "_y = np.linspace(-0.5,0.5,100)\n",
    "ax[0].plot(_x,V[1,0]/V[0,0]*_x, '-', color=colors[0], label=f'PCA-1 = {λ[0]/np.sum(λ):.2f}')\n",
    "ax[0].plot(_y,V[1,1]/V[0,1]*_y, '-', color=colors[-2], label=f'PCA-2 = {λ[1]/np.sum(λ):.2f}')\n",
    "\n",
    "ax[0].axis('equal')\n",
    "ax[0].set_xticks([])\n",
    "ax[0].set_yticks([])\n",
    "ax[0].legend()\n",
    "\n",
    "# perform the projection\n",
    "px = x @ V\n",
    "\n",
    "# this can alternatively be down via direct projection\n",
    "direct = False\n",
    "if direct:\n",
    "    px = np.zeros_like(x)\n",
    "    for n in range(x.shape[0]):\n",
    "        for j in range(2):\n",
    "            px[n,j] = np.dot(V[:,j],x[n,:])\n",
    "    \n",
    "ax[1].scatter(px[:,0],px[:,1], s=1, alpha=0.5, label=r'$\\mathbf{X} \\mathbf{V}$')\n",
    "ax[1].set_xlabel(f'PCA-1 = {λ[0]/np.sum(λ):.2f}')\n",
    "ax[1].set_ylabel(f'PCA-2 = {λ[1]/np.sum(λ):.2f}')\n",
    "ax[1].axis('equal')\n",
    "ax[1].legend()\n",
    "\n",
    "fig.subplots_adjust(wspace=0.25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sci-Kit Learn Implementation\n",
    "\n",
    "Obviously this is useful enough that we don't have to code it up from scratch every time we want the principal components.  It has a convenient implementation in `sklearn`.\n",
    "\n",
    "Our first step is to scale the data (so-called standard or z-scaling) such that:\n",
    "\n",
    "\\begin{equation}\n",
    "\\mathbf{z} = \\frac{\\mathbf{x}-\\langle \\mathbf{x} \\rangle}{\\sqrt{\\langle \\mathbf{x}^{\\top}\\mathbf{x} \\rangle - \\lvert \\langle \\mathbf{x} \\rangle \\rvert^2}} \\, .\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaled_x = scaler.fit_transform(x)\n",
    "\n",
    "model = PCA(n_components=2)\n",
    "\n",
    "# perform the PCA on scaled_x and apply the dimensionality reduction\n",
    "Z_pca = model.fit_transform(scaled_x)\n",
    "\n",
    "λ = model.explained_variance_\n",
    "V = model.components_.T\n",
    "PCAj = model.explained_variance_ratio_\n",
    "\n",
    "print(f'λ = {λ}')\n",
    "print(f'V = {V}')\n",
    "print(f'PCA-j = {PCAj}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"span alert alert-warning\">\n",
    "    Note: To be extra confusing, <code>model.components_.</code> returns the principal vectors as rows! I take the transpose above to compare with our eigenvectors above.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots(1,2,figsize=(8,4))\n",
    "ax[0].scatter(scaled_x[:,0],scaled_x[:,1], s=1, alpha=0.5)\n",
    "\n",
    "_x = np.linspace(-4,4,100)\n",
    "_y = np.linspace(-0.5,0.5,100)\n",
    "ax[0].plot(_x,V[1,0]/V[0,0]*_x, '-', color=colors[0], label=f'PCA-1 = {λ[0]/np.sum(λ):.2f}')\n",
    "ax[0].plot(_y,V[1,1]/V[0,1]*_y, '-', color=colors[-2], label=f'PCA-2 = {λ[1]/np.sum(λ):.2f}')\n",
    "ax[0].axis('equal')\n",
    "ax[0].legend()\n",
    "\n",
    "\n",
    "ax[1].scatter(Z_pca[:,0],Z_pca[:,1], s=1, alpha=0.5, label=r'$\\mathbf{X} \\mathbf{V}$')\n",
    "ax[1].set_xlabel(f'PCA-1 = {PCAj[0]:.2f}')\n",
    "ax[1].set_ylabel(f'PCA-2 = {PCAj[1]:.2f}')\n",
    "ax[1].axis('equal')\n",
    "ax[1].legend()\n",
    "\n",
    "fig.subplots_adjust(wspace=0.25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"span alert alert-success\">\n",
    "<h2>Use Case: Edge States in the Bernevig-Hughes-Zhang Model </h2>\n",
    "\n",
    "In a recent <a href=\"https://journals.aps.org/prb/abstract/10.1103/PhysRevB.109.245115\" title=\"Topological and magnetic properties of the interacting Bernevig-Hughes-Zhang model\">paper</a> we used PCA to confirm the existence of a new topological phase in a simple model of a 2D Topological insulator. In this exercise you will confirm our analysis.\n",
    "    \n",
    " 1. Load the data corresponding to edge state electronic densities from a file.\n",
    " 2. Perform a PCA analysis by copy/pasting code above and predict the dimension of the latent space.\n",
    " 3. Plot a phase diagram where the color at each point corresponds to the first principal component.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the data from disk\n",
    "\n",
    "The data corresponds to the electronic densities at the edge of the BHZ model on a cylinder for different values of the mass $U$ and the interaction strength $m$.  The Cylinder is $4\\times 4$ and we have two oribtals (*s* and *p*) thus each row of our file includes a $U$-value, $m$-value and 32 values of the orbital density. We have a different orbitial density $n_i$ for a total of 317 $(U,m)$ pairs.\n",
    "\n",
    "<!--\n",
    "Uidx = np.where(np.abs(U-5)<0.1)\n",
    "midx = np.where(np.abs(m-1.75)<0.1)\n",
    "np.intersect1d(Uidx,midx)\n",
    "-->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.loadtxt(\"./data/bhz_orbital_density.dat\")\n",
    "U = data[:,0]\n",
    "m = data[:,1]\n",
    "ni = data[:,2:]\n",
    "\n",
    "ni.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's explore the raw DMRG oribtal occupancy data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_ni(ax,density,label=False):\n",
    "    num_sites = density.shape[0] // 2\n",
    "    label1,label2 = '',''\n",
    "    if label:\n",
    "        label1,label2 = '$n_i^s$','$n_i^p$'\n",
    "        \n",
    "    ax.plot(density[:num_sites], color=colors[1], lw=0.2, label=label1)\n",
    "    ax.plot(density[num_sites:], color=colors[-2], lw=0.2, label=label1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots()\n",
    "plot_ni(ax,ni[0],label=True)\n",
    "\n",
    "for j in range(1,ni.shape[0]):\n",
    "    plot_ni(ax,ni[j])\n",
    "\n",
    "plt.ylabel('orbital density')\n",
    "plt.xlabel('site index')\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### I've chosen three potentially interesting values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(ni[146,:], label=f'$U = {U[146]:.1f}, m={m[146]:.2f}$')\n",
    "plt.plot(ni[282,:], label=f'$U = {U[282]:.1f}, m={m[282]:.2f}$')\n",
    "plt.plot(ni[0,:], label=f'$U = {U[0]:.1f}, m={m[0]:.2f}$')\n",
    "\n",
    "plt.legend(loc=(1.0,0.7))\n",
    "plt.ylabel('orbital density')\n",
    "plt.xlabel('site index')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perform the PCA Analysis using the code above\n",
    "\n",
    "Above our input data was only 2-dimensional, here we have data that lives in $\\mathbb{R}^{32}$.  A good first step is to consider only a small number of principal components and test the variance ratios."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = PCA(n_components=10)\n",
    "Z_pca = model.fit_transform(ni)\n",
    "PCAj = model.explained_variance_ratio_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Investigate the explained variance ratios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots(1,1)\n",
    "ax.plot(PCAj,'o')\n",
    "ax.set_yscale('log')\n",
    "ax.set_xlabel('component')\n",
    "ax.set_ylabel('PCA-j')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"span alert alert-warning\">\n",
    "    Almost everything is in the first component! We can see this by plotting the first two components against each other.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots(1,1)\n",
    "ax.plot(Z_pca[:,0],Z_pca[:,1],'o')\n",
    "ax.set_xlabel('component-1')\n",
    "ax.set_ylabel('component-2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Investigate a phase diagram\n",
    "\n",
    "For each value of $m$ and $U$ lets plots the the orbital density projected onto the first principal component."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots(1,1)\n",
    "pd = ax.scatter(U,m,c=Z_pca[:,0], cmap='Spectral_r')\n",
    "plt.colorbar(pd, label='component-1')\n",
    "ax.set_xlabel('$U$')\n",
    "ax.set_ylabel('$m$')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare with the phase diagram in the paper computed from (expensive and conventional) analysis of magnetic and spectral properties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "from IPython.core.display import HTML \n",
    "Image(url=\"https://github.com/DelMaestroGroup/papers-code-interactingBHZmodel/blob/fe417edd2e5db387c5fd5635bdcba50d711b71c7/plots_and_scripts/Phase_Diagram_Plot/Magnetic_phase_diagram_Nx4_BHZ.png?raw=true\", width=400)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"span alert alert-info\" role=\"alert\">\n",
    "    <h3>It Works!</h3>\n",
    "</div>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
